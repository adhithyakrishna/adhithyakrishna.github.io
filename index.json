[{"content":"Solution for leetcode 819 Golang Java  Golang solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  func mostCommonWord(paragraph string, banned []string) string { var sb strings.Builder bannedMap := make(map[string]bool) for _, data := range banned { bannedMap[data] = true } wordCounts := make(map[string]int) var max string for _, ch := range paragraph { if string(ch) == \u0026#34; \u0026#34; || string(ch) == \u0026#34;,\u0026#34; { if sb.Len() \u0026gt; 0 { max = addWordToMap(wordCounts, sb, max, bannedMap) sb.Reset() } } else { if unicode.IsLetter(ch) { sb.WriteString(string(ch)) } } } if sb.Len() \u0026gt; 0 { max = addWordToMap(wordCounts, sb, max, bannedMap) } return max } func addWordToMap(wordCounts map[string]int, sb strings.Builder, max string, bannedMap map[string]bool) string { word := strings.ToLower(sb.String()) wordCounts[word] += 1 if !bannedMap[word] \u0026amp;\u0026amp; wordCounts[word] \u0026gt; wordCounts[max] { max = word } return max }   References:\n Are Maps pass by reference in go ?    Java solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  class Solution { public String mostCommonWord(String paragraph, String[] banned) { Set\u0026lt;String\u0026gt; bannedSet = new HashSet\u0026lt;String\u0026gt;(); for(String ban: banned) { bannedSet.add(ban); } StringBuffer sb = new StringBuffer(); Map\u0026lt;String, Integer\u0026gt; dataMap = new HashMap\u0026lt;String, Integer\u0026gt;(); String max = \u0026#34;\u0026#34;; for(char ch : paragraph.toCharArray()) { if(ch == \u0026#39; \u0026#39; || ch == \u0026#39;,\u0026#39;) { if(sb.length() \u0026gt; 0) { max = addWordToMap(sb, bannedSet, dataMap, max); sb.setLength(0); } } else { if(Character.isAlphabetic(ch)) { sb.append(ch); } } } if(sb.length() \u0026gt; 0) { max = addWordToMap(sb, bannedSet, dataMap, max); } return max; } public String addWordToMap(StringBuffer sb, Set\u0026lt;String\u0026gt; bannedSet, Map\u0026lt;String, Integer\u0026gt; dataMap, String max) { String word = sb.toString().toLowerCase(); if(bannedSet.contains(word)) return max; dataMap.put(word, dataMap.getOrDefault(word, 0) + 1); if(dataMap.get(word) \u0026gt; dataMap.getOrDefault(max, 0)) { return word; } return max; } }       'use strict'; var containerId = JSON.parse(\"\\\"2e3d604510cf6c4c\\\"\"); var containerElem = document.getElementById(containerId); var tabLinks = null; var tabContents = null; var ids = []; if (containerElem) { tabLinks = containerElem.querySelectorAll('.tab__link'); tabContents = containerElem.querySelectorAll('.tab__content'); } for (var i = 0; i 0) { tabContents[0].style.display = 'block'; }  Time complexity : O(N + M)\nSpace complexity : O(N + M) ","description":"Solution for Leetcode 819","id":0,"section":"tech","tags":null,"title":"Most Common word","uri":"https://adhithyakrishna.github.io/tech/leetcode/leetcode819/"},{"content":"Solution for leetcode 977 Golang Java  Golang solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  func sortedSquares(nums []int) []int { result := make([]int, len(nums)) start := 0 end := len(nums)-1 index := len(nums)-1 for start \u0026lt;= end { if(abs(nums[start]) \u0026gt;= abs(nums[end])) { result[index] = nums[start] * nums[start] start++ } else { result[index] = nums[end] * nums[end] end-- } index-- } return result } func abs(x int) int { if(x \u0026lt; 0) { return x * -1; } return x }     Java solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  class Solution { public int[] sortedSquares(int[] nums) { int[] result = new int[nums.length]; int start = 0; int end = nums.length - 1; int index = nums.length - 1; while(start \u0026lt;= end) { if(abs(nums[start]) \u0026lt;= abs(nums[end])) { result[index] = nums[end] * nums[end]; end--; } else { result[index] = nums[start] * nums[start]; start++; } index--; } return result; } public int abs(int data) { return data \u0026lt; 0 ? data * -1 : data; } }       'use strict'; var containerId = JSON.parse(\"\\\"216afdbf67550f67\\\"\"); var containerElem = document.getElementById(containerId); var tabLinks = null; var tabContents = null; var ids = []; if (containerElem) { tabLinks = containerElem.querySelectorAll('.tab__link'); tabContents = containerElem.querySelectorAll('.tab__content'); } for (var i = 0; i 0) { tabContents[0].style.display = 'block'; }  Time complexity : O(N)\nSpace complexity : O(N) ","description":"Solution for Leetcode 977","id":1,"section":"tech","tags":null,"title":"Squares of a sorted array","uri":"https://adhithyakrishna.github.io/tech/leetcode/leetcode977/"},{"content":"  This notes is for the book - High performance java persistence\nMore than half of the application performance bottlenecks originate in the database\nTo get the most out of a persistence layer, the data access logic must resonate with underlying database system.\n High performance java persistence: Data access skill stack   The database server and the connectivity layer Getting familiar with SQL standard and database specific feature can make the difference. The fear of database portability can lead to avoiding highly effective features and lead to a sluggish database layer. All data access frameworks rely on JDBC API for communicating a database server.\nJDBC offers performance optimizations techniques aiming to reduce transaction response time and accommodate more traffic.\nThe application data access layer There are data access patterns that have proven their effectiveness in many enterprise application scenarios.\nThe ORM framework Beside the ORM pattern, the ORM frameworks also employ techniques such as\n Unit of work Identity Map Lazy Loading Embedded Value Entity inheritance Optimistic and Pessimistic locking  Unit of work In Hibernate ORM, a unit of work is a design pattern that keeps track of all changes made to objects during a business transaction. At the end of the transaction, the unit of work sends the changes to the database in a single batch. This reduced the number of database round trips and increase the efficiency of the application.\nIdentity map In Hibernate ORM, the Identity Map is an identity hash map that maps the primary key of an object to the object itself. This is used to ensure that each object is only loaded from the database once per session. The Identity Map is also used to compare objects for equality, using the object\u0026rsquo;s primary key instead of its identity hash code.\nIn Java, the Object class\u0026rsquo;s identityHashCode() method returns an integer that is the identity hash code of the object. The identity hash code is a unique identifier for an object that is not affected by the object\u0026rsquo;s state. Lazy Loading Lazy loading in Hibernate is a technique that delays the loading of associated objects until they are actually needed. This can improve performance by reducing the number of queries that are executed against the database. Ensure that associated objects are unloaded when you are finished with them. If you do not unload the objects, you can increase memory usage.\nEmbeded Value The @Embeddable and @Embedded annotations in Hibernate are used to map an object’s properties to columns in a database table. These annotations are used in combination to allow the properties of one class to be included as a value type in another class and then be persisted in the database as part of the containing class. [1]\nEntity Inheritance Entity inheritance in Hibernate is a feature that allows you to inherit from a parent entity class to a child entity class. This can be useful when you have a common set of properties that you want to share between multiple entities.\nOptimistic and Pessimistic locking   Optimistic locking\nA record is locked only when changes are committed to the database. Optimistic locking is used to avoid concurrent changes on the same data. It allows and detects changes happening at the same time.\n  Pessimistic locking\nA record is locked while it is edited. Pessimistic locking is used to prevent concurrent changes.\n  Optimistic locking is more efficient when update collisions are expected to be infrequent. Pessimistic locking is more efficient when update collisions are expected to occur often.\nORM tools can boost application development speed. The only way to address the inherent complexity of bridging relational data with application domain model is to full understand the ORM framework in use.\nReading the source code is inevitable when facing performance related problems.\nJPA excels in writing data because all DML (Data manipulation language) statements are automatically updates whenever the persistence model changes therefore speeding up the iterative development process.\nThe native query builedr framework Using native SQL is unavoidable in any non-trivial enterprise application. JAP makes it possible to abstract DML statements and common entity retrival queries, but when it comes to reading and processing data, nothing can beat native SQL.\n  JPQL abstracts the common SQL syntax by subtracting database specific querying features, so it lacks support for database specific querying features. (JPQL is the query language defined by JPA)\n  JOOQ embraces database specific query features and provide type-safety query builder which can protect against SQL injection attacks even for dynamic native queries.\n  ","description":"","id":2,"section":"tech","tags":null,"title":"Preface","uri":"https://adhithyakrishna.github.io/tech/high_performance_java_persistence/introduction/introduction/"},{"content":"  This notes is for the course Kuberenetes for absolute beginners\nIntroduction Container orchestration system = Container + Orchestration\nWhat are containers  Containers have their own processes, network and mounts Multiple containers can share the underlying operating system kernel.  Docker (most popular container technology).\nProblems before containers  Application component and services being incompatible with underlying OS. Compatibility between services, libraries and dependencies on the OS. Compatibility checks had to be make during every component upgrade AKA matrix from hell. On-boarding a new developer / setting up a local instance was difficult.  With docker  Each component can run in its own container with its own libraries and dependencies. Docker is compatible with any operating systems. On-boarding a new developer / setting up a local instance is very easy.  Os components and responsibilities All operating systems consists of two important components\n OS Kernel Software  Os kernel is responsible for interacting with underlying hardware. Custom software differentiates operating systems from each other. Docker container shares the underlying kernel of docker host. Docker is not meant to virtualise and run different operating systems on the same hardware. The main purpose is to containerize and ship them.\nDocker vs Virtual machines  Containers vs Virtual Machine: https://www.udemy.com/course/learn-kubernetes/   In case of docker we have\n Underlying hardware infrastructure. Operating system Docker installed on the OS (which is responsible for managing the containers that run with libraries and dependencies).  In case of virtual machine\n Underlying hardware infrastructure Operating system Hypervisor (ESX or virtualization) Virtual machine Virtual machine has its own OS inside Dependencies Application  The overhead causes higher utilization of underlying resources because there are multiple operating systems and kernel running. The Virtual machine is heavy and consume high disk space (Gigabytes) whereas Docker containers a re light weight and are usually mega bytes in size.\nDocker containers boot up faster (within seconds). VM takes minutes to boot up as it needs to boot up the entire OS.\nDocker has less isolation as more resources (like kernel) are shared between containers.\nFor VMs, there is complete isolation. Since VM does not directly rely on underlying OS or kernel, we can run different OS such as linux / windows based on same hypervisor.\n\u0026ldquo;Hypervisors and containers are used for different purposes. Hypervisors are used to create and run virtual machines (VMs), which each have their own complete operating systems, securely isolated from the others. In contrast to VMs, containers package up just an app and its related services. This makes them more lightweight and portable than VMs, so they are often used for fast and flexible application development and movement.\u0026rdquo;\nReference : https://www.vmware.com/topics/glossary/content/hypervisor.html?resource=cat-1023790256#cat-1023790256 Image vs Containers An image is a package or a template that is used to create one or more containers.\nContainers are running instances of that image that are isolated and have their own environments and set of processes.\nAdvantage of containers Traditionally, developers developed applications and hand it over to Ops team to deploy and manage it in production environments along with some instructions. If they hit an issue, they would have to work with developers to resolve it.\nWith docker, major portion of this infrastructure setup is now in the hands of developers in form of Docker file. The instructions that were put together previously (handed off to the ops team) can now put together easily into a Docker file (to create an image for their application). The image can run on any container platform and is guaranteed to run the same way everywhere. Ops team can now use the image to deploy the application. Since OPS team are not modifying it, it continues to work the same when deployed in production.\n","description":"Introduction to Kubernetes","id":3,"section":"tech","tags":null,"title":"Introduction","uri":"https://adhithyakrishna.github.io/tech/k8s_for_absolute_beginners/introduction/"},{"content":"  Civil War It was a blissful morning until I heard a blaring noise,\nA noise of an explosion followed by shrill shouts and cries.\nI saw a ginourmous object being dropped down from an hover,\nBefore I could react, everything was already over.\nAs I regained my bleared vision, I couldn\u0026rsquo;t believe my eyes,\nAn enormous blanket of smog covered the scarlet sky.\nEverything was tattered and demolished from the intense mar,\nThey had destroyed my entire village in the name of civil war.\nYou know,\nOur ancestors inhabited this village and their origins cannot be dated,\nMost ancient, revered and civilized that their sanctity cannot be stated.\nThe vast stretch of this fertile land is now wilted and withered,\nMy beautiful village that once flourished lies soulless and desimated.\nThe survivors were forced in to a refugee camp, a so-called safe place,\nThousands of us were coerced to survive in this constricted space.\nIn the name of displaced refugees, we were treated like an animal herd.\nOur desperate cries for food and neccessities were ignored or left unheard.\nMy interim stay at this camp were the most unbearable days,\nEach and every one of us suffered abuse in numerous ways.\nWomen and hapless infants were the most exploited victims,\nThese tortures was carried out to fulfill the dictator\u0026rsquo;s dictums.\nThe horrendous act of inhumanity increased each day,\nRape and brutal abuse evolved into a cold-blooded slay.\nLoosing my bloodmates one by one was something my heart couldn\u0026rsquo;t bear,\nAll of them left this earth having their soul imprinted with fear.\nAs the seconds of darkness fleeted incessantly, arrived my final day.\nI had not choice but to participate in this tragic play.\nA pointy large bullet was fired, it pierced right in my heart,\nI wish to leave back this final message as my soul starts to depart.\nFinal message ------------- \u0026quot;You were born on this earth bringing nothing, And that is how you'll leave. Your debauched ways of acquiring something, Will definitely end up in grieve. Everyone was born with an empty hand, No person was alloted a permanent berth. This entire Earth is mother nature's land, And this is the eternal truth. No force except the nature can, determine your birth or death. Remember, Every soul killed here will revive again like the beatified holy man of Nazareth.\u0026quot; -Adhi\nThis poem is based on real life incidents Sri lankan civil war and is dedicated to those who lost their lives to the WAR.\n","description":"","id":4,"section":"literature","tags":null,"title":"Civil War","uri":"https://adhithyakrishna.github.io/literature/poems/civilwar/"},{"content":"This post was inspired by an awesome tech talk by Florian Patan at GopherCon UK in 2018 where he goes over creating a goservice in 30 minutes. The interesting take away from the talk was the use of dependency injection to insert a logger instance into the handler.\nMy aim for this article is to dissect dependency injection into smaller chunks to understand how it works.\nThe initial code for the project is given below. (main.go)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; ) func main() { logger := log.New(os.Stdout, \u0026#34;log \u0026#34;, log.LstdFlags|log.Ltime|log.Lshortfile) mux := http.NewServeMux() mux.HandleFunc(\u0026#34;/\u0026#34;, func(rw http.ResponseWriter, r *http.Request) { logger.Printf(\u0026#34;Inside handler\u0026#34;) rw.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/plain; charset=utf-8\u0026#34;) rw.WriteHeader(http.StatusOK) rw.Write([]byte(\u0026#34;Hello world\u0026#34;)) }) http.ListenAndServe(\u0026#34;:8080\u0026#34;, mux) }   Note that new instance of logger has been initialised in line 10.\nThe flags log.LstdFlags define which text to prefix to each log entry generated by the Logger.\nlog.Ltime represents the time at which the log was generated and the log.Lshortfile represents the name of the file from which the log was printed.\nOn executing the file we get the following output.\nlog 2021/02/01 16:44:03 main.go:15: Inside root\nlog 2021/02/01 16:44:03 main.go:15: Inside root\nThe logger instance was generated in main.go file. Suppose we want to create a new router called home. Instead of creating a new instance of logger we can simply inject it into the router. This is where the dependency injection comes into the picture.\nWe create a new package that handles the logic of /home route. The new package is home package. In the home.go file, we add the following code.\nDependency Injection Step 1 : Create a Handler of type struct and initialise a logger variable of the type log.Logger 1 2 3  type Handlers struct { logger *log.Logger }   Step 2 : Create a function constructor to initialise the logger variable. It returns an address of the initialised Handler. 1 2 3 4 5  func NewHandlers(logger *log.Logger) *Handlers { return \u0026amp;Handlers{ logger: logger, } }   Step 3: Handle method takes implementation of http.Handler interface as the second argument, hence serveHttp method has been intialised with the code logic (logger is accesses inside the function). 1 2 3 4 5 6  func (h *Handlers) ServeHTTP(rw http.ResponseWriter, r *http.Request) { h.logger.Printf(\u0026#34;Inside home\u0026#34;) rw.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/plain; charset=utf-8\u0026#34;) rw.WriteHeader(http.StatusOK) rw.Write([]byte(\u0026#34;from home page\u0026#34;)) }   Step 4: Instance of handler is intialised and assigned to a new variable passed to the /home router 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  func main() { logger := log.New(os.Stdout, \u0026#34;log \u0026#34;, log.LstdFlags|log.Ltime|log.Lshortfile) h := homePage.NewHandlers(logger) mux := http.NewServeMux() mux.HandleFunc(\u0026#34;/\u0026#34;, func(rw http.ResponseWriter, r *http.Request) { logger.Printf(\u0026#34;Inside root\u0026#34;) rw.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/plain; charset=utf-8\u0026#34;) rw.WriteHeader(http.StatusOK) rw.Write([]byte(\u0026#34;Hello world\u0026#34;)) }) mux.Handle(\u0026#34;/home\u0026#34;, h) http.ListenAndServe(\u0026#34;:8080\u0026#34;, mux) }   The output is\nlog 2021/02/01 18:48:55 main.go:18: Inside handler\nlog 2021/02/01 18:49:04 home.go:19: Inside home\nWe can clearly see that the logger instance is initialized once and injected into wherever necessary.\nFind the source code here https://github.com/adhithyakrishna/go-dependency-injection ","description":"An article about dependency injection in golang","id":5,"section":"tech","tags":null,"title":"Dependency injection in golang","uri":"https://adhithyakrishna.github.io/tech/golang/dependencyinjection/"},{"content":"  Contains Notes for the book Java concurrency in practice - https://jcip.net/)\nRunning a single program at a time was inefficient use of expensive and scarce computer resources\nProcesses Processes are isolated, independently executing programs to which operating system allocates resources such as memory, file handles and security credentials.\nProcesses communicate with one another through a variety of coarse-grained communication mechanism: sockets, signal handlers, shared memory, semaphores and fils.\nMulti program execution Several motivating factors led to the development of OS that allowed multiple programs to execute simultaneously.\n Resource Utilization: Programs have to sometimes wait for external operations such as input or output. While waiting for external operations, it is more efficient to use the wait time to let another program run. Fairness: Multiple users and programs may have equal claims on machine\u0026rsquo;s resources. It is preferable to let the users share the resources via finer-grained time slicing rather than keeping them wait until the current program completes execution. Convenience: It is easier to write several program that each perform a single task and have them coordinate with each other as necessary than to write a single program that performs all the tasks.\nThe same factors that motivated the development of processes also motivated the development of threads.  Threads Threads are light weight processes, most operating systems treat threads, not processes as the basic unit of scheduling.\n Allow multiple streams of program control flow to co-exist within a process. They share a process wide resource such as memory and file handles. Each thread has its own program counter, stack and local variables. Multiple threads within the same program can be scheduled simultaneously on multiple CPUs.   Single vs Multithreaded: Java Concurrency in practice   In absence of explicit coordination threads execute simultaneously and asynchronously with respect to one another. Since threads share the memory address space of their owning process, all threads within a process have access to same variables and allocate objects from the same heap allowing finer grained data sharing than inter-process mechanisms.\nBut without explicit synchronization to coordinate access to shared data, a thread may modify variables that another thread is in the middle of using, with unpredictable results.\n","description":"Contains Notes for the book Java concurrency in practice - https://jcip.net/)","id":6,"section":"tech","tags":null,"title":"Ch01 - Introduction","uri":"https://adhithyakrishna.github.io/tech/java_concurrency_in_practice/introduction/introduction/"},{"content":"  The JDBC API provides common interface for communicating to the db server. To communicate to a db server, a Java program must first obtain a java.sql.connection.. java.sql.Driver is the actual db connection provider. java.sql.DriverManager provides the more convenience since it can also resolve the JDBC driver associated with the current db connection URL.\nDriverManager Every time the getConnection() method is called, the driver manager will request a new physical connection from the underlying driver.\nIn a two-tier architecture, the application is run by single user and each instance uses a dedicated db connection. Each database server, based on the underlying resources can only offer a limited number of connections.\n High performance java persistence: DriverManager connections   DataSource Intermediate data layer In a three-tier architecture, the middle tier acts as a bridge between user requests and various data sources (relation databases, messaging queues).\nHaving this intermediate layer has numerous advantages. It acts as a db connection buffer that can mitigate user request traffic spikes by increasing request response time, without depleting db connections or discarding incoming traffic. The application server provides only logical connections (proxies or handles), so it can intercept and register how the client API interacts with the connection object.\nA three-tier architecture can accommodate multiple data sources or messaging queue implementations. To span a single transaction over multiple sources of data, a distribution transaction manager becomes mandatory. In Java Transaction API (JTA), the transaction manager must be aware of the logical connections the client has acquired as it has to commit or roll them back according to the global transaction outcome.\nBy providing logical connections, the application server can decorate the db connection handles with JTA transaction semantics.\njava.sql.DriverManager - is a physical connection factory.\njava.sql.DataSource - is a logical connection provider interface.\n High performance java persistence: Datasource without connection pooling    Application data layer asks the DataSource for a db connection. The DataSource will use the underlying driver to open a physical connection. A physical connection is created, and TCP socket is opened. The DataSource doesn\u0026rsquo;t wrap the physical connection and simply lends it to the application layer. The application executes statements using acquired db connection. When the connection is no longer needed, the application closes the physical connection along with the underlying TCP socket.  Opening and closing db connection is a very expensive operation. Reusing them has the following advantages\n It avoids both the db and driver overhead for establishing a TCP connection. it prevents destroying the temporary memory buffers associated with each db connection. reduces client-side JVM object garbage.   When using a connection pooling solution, the connection acquisition time is between two and four orders of magnitude smaller.\nWhy is pooling much faster?  High performance java persistence: Connection acqusition flow    When a connection is being requested, the pool looks for unallocated connections. If the pool finds a free one, it handles it to the client. If there is no free connection, the pool tries to grow to its maximum allowed size. If the pool already reached its maximum size, it will retry several times before giving up with a connection acquisition failure exception. When the client closes the logical connection, the connection is released and returns to the pool without closing the underlying physical connection.  Most connection pooling exposes a DataSource implementation that either wraps an actual database specific DataSource or the underlying DriverManager utility.\n High performance java persistence: Logical connection lifecycle     The connection pool offers a proxy or a handle instead of a physical connection to the client.\n  When the connection is in use, the pool changes its state to allocated. (To prevent two concurrent threads from using the same db connection).\n  The proxy intercepts the connection close method call, it notifies the pool to change the connection state to unallocated.\nConnection pool can act as a bounded buffer for the incoming connection requests. If there is a traffic spike, the connection pool will level it, instead of saturating all the available db resources.\nConfiguring the right pool size is not a trivial thing to do. Provisioning the connection pool requires understanding\n Application-specific database access patterns Connection usage monitoring    Whenever the number of incoming requests surpass the available handlers, there are two options to avoid system overloading.\n Discarding the overflowing traffic (affects availability) Queuing requests and wait for busy resources to be available (increasing response time)  Discarding the surplus traffic is usually a last resort measure. Most connection pooling solutions first attempt to enqueue overflowing incoming requests. By putting an upper bound on the connection request wait time, the queue is prevented from growing indefinitely and saturating the application server resources.\nFor a given incoming request rate, the relation between queue size and average enqueuing time is given by one of the most fundamental laws of queuing theory.\nQueuing theory capacity planning Little\u0026rsquo;s Law Little Law is a general purpose equation applicable to a queuing system that is in a stable state.\nAssuming that the application-level transaction uses the same db connection through its life-cycle,\n If the average transaction response time is 100 milliseconds (0.1 seconds). If the average connection acquisition rate is 50 requests per second.  The average number of connection requests in the system is 50 * 0.1 = 5 connection requests. (Both the requests being serviced and the ones waiting in the queue).\nA pool size of 5 can accommodate the average incoming traffic without having to enqueue any connection requests.\nIf the pool size is set to 3, then on average 2 requests are enqueued and waiting for connections to be available.\nLittle\u0026rsquo;s Law operates with long-term averages and might not be suitable when taking into consideration the intermittent traffic bursts. In real-life scenario, the connection pool must adapt to short-term traffic spikes. It is important to consider the actual connection pool throughput. Queuing theory  If the average service time is 100 milliseconds (0.1 seconds). If the average connection acquisition rate is 50 requests per second.\nWe concluded that the pool can offer at most 5 connections (there are at most 5 in-service requests).  In queuing theory, throughput is represented by the departure rate (μ), for a connection pool, it represents the number of connections offered in a given unit of time:\n(μ) = Ls / Ws\nWhere,\nLs = Pool size\nWs = Connection lease time\n If the arrival rate outgrows the connection pool throughput, the overflowing requests must wait for connections to become available.\nA one second traffic burst of 150 records is handled as\n The first 50 requests can be served in the first second. The following 100 requests are first enqueued and processed in the following two seconds.  For a constant throughput, the number of enqueued connection requests is proportional to the connection acquisition time.\nThe total time required to process the spike is given by the formula:\nW = Lspike / (μ)\nWhere,\nLspike is the total number of requests in any given spike.\n(μ) is the throughput\n Assuming there is a traffic spike of 250 requests per second for 3 seconds. Then the Lspike is 750 requests. If the throughput (μ) is 50 requests / second, Then the total time to process all the records is 15 seconds.\n","description":"","id":7,"section":"tech","tags":null,"title":"JDBC connection management","uri":"https://adhithyakrishna.github.io/tech/high_performance_java_persistence/jdbc_and_database_essentials/jdbc_connection_management/"},{"content":"  This notes is for the book - High performance java persistence\nAn enterprise application needs to store and retrieve as much data and as fast as possible. In application performance management, the two most important metrics are,\n response time throughput  The lower the response time, the more responsive the application becomes. Scaling is about maintaining low latencies while the system load increases.\nResponse time is measure of performance.\nThroughput is the measure of scalability.\nResponse time and throughput The transaction response time is measured as the time it takes to complete a transaction. So it encompasses the following segments\n Database connection acquisition time Time taken to send all the statements over the wire Execution time for all the statements Time taken for sending the results back to the db client The time the transaction is idle due to application level computations prior to releasing the db connection.  Throughput is the rate of completing incoming load. In db context, throughput can be calculated as the number of transactions executed within a time interval.\nX = transaction count / time\nLowering the execution time of the transaction, the system can accommodate more requests.\nThe throughput measured from a single request becomes baseline for further concurrency based improvements.\nIf the system was to scale linearly, adding more db connections would yield a proportional throughput increase. But, due to contention on db resources and cost of maintaining coherency across multiple concurrent db sessions, the relative throughput follows a curve instead of a straight line.\nThe Universal Scalability law can approximate the maximum relative throughput (system capacity) in relation to the number of load generators (database connections). USL formula is an extension to the widely known Amdahl\u0026rsquo;s law.\n Contention coefficient - Serializable portion of the data processing routine. Contention has the effect of leveling up the scalability Coherency coefficient - Cost of maintaining consistency across all concurrent db sessions. Coherency is responsible for the inflection point in the scalability curve. Its effect becomes more significant as the number of concurrent sessions increases.  When coherency coefficient is zero, USL overlaps the Amdahl\u0026rsquo;s law.\nThe number of db connections for which the system hits its maximum capacity depends on USL coefficients solely.\nDatabase connections boundaries Every connection requires a TCP socket from client (app) to the server (db).\nThe total number of connections offered by a database server depends on the underlying hardware resources.\nPostgreSQL is implemented using a simple \u0026ldquo;process per user\u0026rdquo; client/server model. In this model there is one client process connected to exactly one server process. [1]\nDatabase system internals reveal tight dependency on CPU, Memory and Disk resources. The database uses buffer pool to map into memory the underlying data and index pages. Changes are first applied in memory, and flushed to disk in batches to achieve better write performance.\nThe db might still need to access disk to fetch associated data pages into memory buffer pool. To provide data consistency, locks are used to protect data blocks from being updated concurrently. This means that a high-throughput db application will experience contention CPU, Memory, Disks and Locks. When all the db server resources are in use, adding more work will only increase the contention, thereby reducing the throughput.\nResources might get saturated due to improper system configuration, the first step to improve the system throughput is to tune it according to current data access patterns\nCapacity planning is a feedback driven mechanism, and it requires constant application monitoring. Any optimization must be reinforced by application performance metrics.  Scaling up and scaling out Scaling is the effect of increasing the capacity by adding more resources.\n Scaling vertically - Adding more resource to a single machine. Scaling horizontally - Increasing the number of available machines.  Traditionally, scaling vertically has been the preferred way of increasing the database capacity. Distributed systems are much more complex to manage than the centralized ones, hence scaling horizontally is more challenging. Instead of a high performance server, for the same price, one could buy multiple commodity machines whose sum of available resources is much greater than the single dedicated server.\nNo matter how powerful a single dedicated server may be, it is still a single point of failure.\nMaster-Worker replication When read / write ratio is high, a Master - Worker replication scheme is suitable for increasing availability. The master is the system of record and the only node accepting writes. All changes recorded by master node are replayed onto workers.\nA binary replication uses Master node WAL (Write Ahead Log), while a statement-based replication replays on the worker machine, the exact statements executed on master.\n Asynchronous replication is common when there are more worker nodes to update. The worker nodes are eventually consistent. In case a master node crashes, there is a cluster-wide voting process to elect the new master. This is usually node that has the most recent update record from the list of all available workers. The Asynchronous replication topology is also referred to as warm standby because the election process doesn\u0026rsquo;t happen instantaneously. When only asynchronous workers are available, the newly elected worker might lag behind failed master, in which consistency and durability are traded for lower latency and higher throughput. Synchronous replication allows system to ensure there is data consistency in case of Master node failure, since the synchronous worker is an exact copy of the master. The synchronous Master-worker replication is called hot standby topology because the synchronous worker node is readily available for replacing the master node.  Aside from eliminating the single point of failure, db replication can also increase the throughput. Since the worker node only accept read-only transactions, therefore routing read traffic away from the Master node. This reduced the master node resource contention, and lowers the read-write transaction response time. If the master node can no longer keep up with ever increasing read-write traffic, a multi master replication might be a better alternative.\nMulti-Master replication In this scheme, all nodes are equal and can accept both read-only and read-write transactions. Splitting the load among multiple nodes can not only increase the transaction throughput but also reduce the response time.\nEnsuring data consistency is challenging in Multi-Master replication scheme because there is no longer a single source of truth. Same data can be modified concurrently on separate nodes, so there is a possibility of conflicting updates. The replication schema can either avoid conflicts or detect them and apply automatic conflict resolution algorithm.\nA two phase commit protocol can be used to enlist all participating nodes in one distributed transaction. This allows all the nodes to be in-sync all the time, at the cost of increasing the transaction response time (by slowing down write operations).\nIf nodes are separated by WAN (Wide area network), synchronization latencies can increase dramatically. If one node is no longer reachable, the synchronization could fail and the transaction will roll back on all masters.\n Synchronous replication - Although it provides data consistency, might incur high transaction response time. Asynchronous replication - Although it provides better through put, it has to do it at the cost of having to resolve conflicts. The async master-master replication requires a conflict detection and automatic conflict resolution algorithms. When a conflict is detected, that automatic resolution tries to merge the two conflicting branches, and if it fails, manual intervention is required.  Sharding When the data size grows beyond the overall capacity of replicated multi-node environment, splitting data becomes unavoidable. Sharding means distributing data across multiple nodes, so that each node has only subset of data.\nAs opposed to horizontal partitioning (distribute data across multiple tables within the same db server), sharding requires a distributed systems topology so that the data is spread across multiple machines.\nEach shard must be self contained (each transaction should use data only from a single shard). Joins across shards is usually prohibited because the cost of distributed locking and the networking overhead would lead to long transaction response times.\nBy reducing the data size to be store in each node, indexes will require less space and can it into main memory. With less data to query, the transaction response time can get shorter.\nEach sharding topology includes at least two separate data centers. Each can serve a dedicated geographical region, so that the load is balanced across geographical regions. Smaller tables can be duplicated on each nodes to increase performance. Asynchronous replication mechanism can be employed to keep the shards in-sync.\nSharding is usually the last resort strategy, after exhausting all other available options such as\n Optimizing data layer to deliver lower transaction response time Scaling each replicated node to a cost-effective configuration Adding more replicated nodes until synchronization latencies start dropping below acceptable threshold.  ","description":"","id":8,"section":"tech","tags":null,"title":"Performance and Scaling","uri":"https://adhithyakrishna.github.io/tech/high_performance_java_persistence/introduction/performance_and_scaling/"},{"content":"  Concurrency is more about achieving thread-safety, than it is about creating \u0026amp; managing threads. Those are mechanisms, but at its core, concurrency aims to encapsulate shared mutable state from uncontrolled concurrent access.\nState of an object An object\u0026rsquo;s state encompasses any data that can affect its externally visible behavior. An object\u0026rsquo;s state is its data, stored in state variables such as instance or static fields.\nAn object\u0026rsquo;s state may include fields from other dependent objects. Example a HashMap\u0026rsquo;s state is partially stored in the Hash Map object itself, but also in many Map.Entry objects.\nShared Variable: could be accessed by multiple threads.\nMutable: Value could change during its lifetime.\nThread safety Thread-safety is about trying to protect the data from uncontrolled concurrent access. Whether an object needs to be thread-safe depends on whether it will be accessed from multiple threads. This is a property of how an object is used in a program and not what it does.\nMaking an object thread-safe requires using synchronization to coordinate access to its mutable state. Failing to do so could result in data corruption and other undesirable consequences.\nWhen more than one thread accesses a given state variable and one of them might write to it, they all must coordinate their access to it using synchronization.\nThe primary mechanism for synchronization in Java is the synchronized keyword, which provides exclusive locking, but the term \u0026lsquo;synchronization\u0026rsquo; also includes the use of\n1)volatile variables\n2) Explicit locks\n3) Atomic variables\nA program that omits the needed synchronization might appear to work, passing its test and performing well for years, but it is still broken and may fail at any moment. If multiple threads access the same mutable state variable without appropriate synchronization, the program is broken and there are three ways to fix it:\n Don\u0026rsquo;t share the state variables across threads. Make the state variable immutable. Use synchronization whenever accessing the state variable.  \nIf it easier to design a class to be thread-safe than to retrofit it for thread safety later.\nObject oriented techniques can help write well-organized, maintainable classes - such as encapsulation and data hiding - can also help create thread-safe classes.\nJava does not force developers to encapsulate state - A state can be stored in public fields or public static fields or publish a reference to an otherwise internal object, but the better encapsulated the program state is, the easier it is to make the program thread-safe. The less code that has access to a particular variable, the easier it is to ensure that all of it uses proper synchronization.\nWhen designing thread-safe classes, good object-oriented techniques - encapsulation, immutability and clear specification of invariant are helpful.\nInvariant means something that should stick to its conditions no matter whatever changes or whoever uses/transforms it.\n There might be times when good OO design techniques are at odds with real-world requirements.\n Rules of good design need to be compromised for the sake of performance or for the same of backward compatibility with legacy code. Abstraction and Encapsulation are at odds with performance - (although not nearly as often).  It is a good practice to make the code right and then make it fast. Pursue optimization only if needed and those same measurements tell you that your optimizations actually made a difference under realistic conditions.\nA program that consists entirely of thread-safe classes may not be thread-safe. The concept of a thread-safe class makes sense only if the class encapsulates its own state.\nThread safety may be a term that is applied to code, but it is about state, and it can be applied to the entire body of code that encapsulates its state, which may be an object or an entire program. What is Thread Safety? The heart of any reasonable definition of thread safety is its concept of correctness.\nCorrectness means that a class conforms to its specification. A good specification defines in-variants constraining an object\u0026rsquo;s state and post - conditions describing the effects of its operations.\nA class is thread-safe if it behaves correctly when accessed from multiple threads, regardless of the scheduling or interleaving of the execution of those threads by the runtime environment, and with no additional synchronization or other coordination on the part of the calling code. If an object is correctly implemented, no sequence of operations - calls to public methods and reads or writes of public fields - should be able to violate any of its invariant or post-conditions.\nNo set of operations performed sequentially or concurrently on instances of a thread-safe class can cause an instance to be an invalid state.\nFrameworks like servlet frameworks create threads and call components from those threads, leaving developers with making the components thread-safe.\nStateless servlet 1 2 3 4 5 6 7  public class StatelessFactorizer implements Servlet { public void service(ServletRequest req, ServletResponse resp) { BigInteger i = extractFromRequest(req); BigInteger[] factors = factor(i); encodeIntoResponse(resp, factors); } }   StatelessFactorizer in the above example has no fields and references no fields from other classes.\nThe transient state for a particular computation exists solely in local variables that are stored on thread\u0026rsquo;s stack and are accessible only to the executing thread.\nOne thread accessing a StatelessFactorizer cannot influence the result of another thread accessing the same StatelessFactorizer because two threads do not share state. It is as if they were accessing different instances.\nSince the actions of a thread accessing a stateless object cannot affect the correctness of operations in other threads, stateless objects are thread-safe.\nStateless objects are always thread safe. If its only when servlet wants to remember things from one request to another that the thread safety requirement becomes and issue.\nAtomicity Adding one element of state to a stateless object would make it not thread-safe, even though it worked just fine in a single-threaded environment.\nIn this example, a variable count is introduced to keep track of the number of request processed. Though this may work just fine in a single threaded environment, it is susceptible to lost updates.\ncount variable as noted previously, despite its shorthand syntax has 3 discrete operations (read-modify-write operation).\n Fetch the current value. Add one to it. Write the new value back.  1 2 3 4 5 6 7 8 9 10  public class UnsafeCountingFactorizer implements Servlet { private long count = 0; public long getCount() { return count; } public void service(ServletRequest req, ServletResponse resp) { BigInteger i = extractFromRequest(req); BigInteger[] factors = factor(i); ++count; encodeIntoResponse(resp, factors); } }   If two threads increment the counter simultaneously, without synchronization, the counter would not give an accurate result. The possibility of incorrect results in the presence of unlucky timing is called a race condition.\nRace conditions A race condition occurs when the correctness of the computation depends on lucky timing (relative timing or interleaving of multiple threads by runtime).\ncheck-then-act check-then-act is a type of race condition that uses a potentially stale observation to make a decision or perform a computation. The observation could have even become invalid between the time it was observed and the time an action was take on it.\nExample: Lazy initialization\n1 2 3 4 5 6 7 8 9  @NotThreadSafe public class LazyInitRace { private ExpensiveObject instance = null; public ExpensiveObject getInstance() { if (instance == null) instance = new ExpensiveObject(); return instance; } }   The goal of lazy initialization is to defer initializing an object until it is actually needed at the same time making sure that it is initialized only once.\nThe getInstance() method checks whether the expensive object has been initialized, in this case it returns the existing instance, otherwise it creates a new instance and returns it retaining a reference so that it can be used for future invocations (to avoid expensive code paths).\nWhen two threads A and B execute getInstance() at the same time. \u0026lsquo;A\u0026rsquo; sees that instance is null and initiate a new object. \u0026lsquo;B\u0026rsquo; also checks if instance is null. The state of instance depends on\n Timing (which is unpredictable) Vagaries of scheduling How long A takes to initiate the ExpensiveOject and set the instance field.  \u0026lsquo;B\u0026rsquo; may again initialize a new instance even though getInstance() was supposed to return the same instance.\nRace conditions does not always result in failure but can cause serious problems. As an example, different instances from multiple invocation could cause registrations to be lost or multiple activities to have inconsistent views of the set of registered objects.\nread-modify-write As we saw previously ++count is not an atomic operation. To properly increment the counter, the current thread must know its previous value and make sure no other thread is changing or using the value in the middle of the update.\nCompounded Actions Both read-modify-write and check-then-act (collectively referred to as compound actions) contains a sequence of operations that need to be atomic or indivisible, relative to other operations on the same state (i.e) sequences of operations that must be executed atomically in order to remain thread safe.\nTo avoid race conditions, there must be a way to prevent other threads from using a variable while we are in the middle of modifying it, so we can ensure that other threads can observer or modify the state only before we start or after we finish but not in the middle.\nAn atomic operation is one that is atomic with respect to all operations, including itself, that operate on the same state.\nOperations A and B are atomic with respect to each other if, from the perspective of a thread t1 executing operation A, when another thread t2 executes B, either all of B has executed or none of it has.\n The CountingFactorizer can be made atomic by using java.util.concurrent.atomic package that contains atomic variable classes for effecting atomic state transition on numbers and object references.\nBy replacing the long counter with an AtomicLong, all actions that access the counter state are atomic. Because the state of the servlet is the state of the counter and the counter is thread-safe the servlet is once again thread-safe.\nWhere practical, use existing thread-safe objects, like AtomicLong, to manage class\u0026rsquo;s state. It is simpler to reason about the states and state transition for existing thread-safe objects than it is for arbitrary state variables, and makes it easy to maintain and verify the thread safety. Locking Adding one more thread-safe state variable to the class will not make it thread-safe\nThe definition of thread safety requires that invariant be preserved regardless of timing or interleaving of operations in multiple threads.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  public class UnsafeCachingFactorizer implements Servlet { private final AtomicReference\u0026lt;BigInteger\u0026gt; lastNumber = new AtomicReference\u0026lt;BigInteger\u0026gt;(); private final AtomicReference\u0026lt;BigInteger[]\u0026gt; lastFactors = new AtomicReference\u0026lt;BigInteger[]\u0026gt;(); public void service(ServletRequest req, ServletResponse resp) { BigInteger i = extractFromRequest(req); if (i.equals(lastNumber.get())) encodeIntoResponse(resp, lastFactors.get() ); else { BigInteger[] factors = factor(i); lastNumber.set(i); lastFactors.set(factors); encodeIntoResponse(resp, factors); } } }   The above example is responsible for improving the performance of the servlet by caching the most recently computed result, just in case two consecutive clients request factorization of the same number. To implement this, the last number factored and its factors needs to be remembered.\nUsing atomic references, both lastNumber and lastFactor cannot be updated simultaneously. Even though each call to set is atomic, there is still a window of vulnerability when one has been modified and the other has not, and during that time, other threads could see that the invariant does not hold. Similarly, two values cannot be fetched simultaneously, between the time when thread A fetches the two values, thread B could have changed them and again A may observe that the invariant does not hold.\nTo preserve state consistency, update related state variables in a single atomic operation. Reentrancy Reentrancy means that locks are acquired on a per-thread rather than per invocation basis.\nWhen a thread requests a lock that is already held by another thread, the requesting thread blocks. But since intrinsic locks are re entrant, if a thread tries to acquire a lock that it already holds, the request succeeds.\nReentrancy is implemented by associated each lock with an acquisition count and an owning thread.\nWhen a thread acquires the lock (previously unheld lock) the JVM records the owner and sets the acquisition count to 1. If the same thread acquires the lock again, the count is incremented and when the owning thread exists the synchronized block the count is decremented. When the counter becomes zero, the lock is released.\nReentrancy facilitates encapsulation of locking behavior and thus simplifies the development of object-oriented concurrent code.\n1 2 3 4 5 6 7 8 9 10 11  public class Widget { public synchronized void doSomething() { ... } } public class LoggingWidget extends Widget { public synchronized void doSomething() { System.out.println(toString() + \u0026#34;: calling doSomething\u0026#34;); super.doSomething(); } }   In the above example calls to the super class method would deadlock without re-entrant locks. Re-entrancy saves us from this deadlock situation.\nGuarding state with locks Because locks enable serialized access (Serializing access means threads take turns accessing the object exclusively, rather than doing concurrently) to the code paths they guard, we can use them to construct protocols for guaranteeing exclusive access to the shared state.\nCompound actions on shared state such as incrementing hit counter (read-modify-write) or lazy initialization (check-then-act) must be atomic to avoid race conditions.\nJust wrapping the compound action with a synchronized block is not sufficient.\n If synchronization is used to coordinate access to a variable, it is needed everywhere that variable is accessed. When using locks to coordinate access to a variable, the same lock must be used wherever that variable is accessed.  For each mutable state variable that may be accessed by more than one thread, all access to that variable must be performed with the same lock held. In this case, we say that the variable is guarded by that lock. Every shared, mutable variable should be guarded by exactly one lock and should be made clear to maintainers which lock it is. A common locking convention is to encapsulate all mutable state within an object to protect it from concurrent access by synchronizing any code path that access mutable state using the object\u0026rsquo;s intrinsic lock. It is easy to subvert this locking protocol accidentally by adding a new method or code path and forgetting to use synchronization.\nAdding synchronization to every method is not enough. For example, put-if-absent operation has a race condition, even though both contains and add are atomic (When multiple operations are combined into a compound action).\n1 2  if (!vector.contains(element)) vector.add(element);   At the same time, synchronizing every method can lead to liveliness or performance problems.\nLiveness and Performance It is easy to improve the concurrency of a component while maintaining thread safety by narrowing the scope of synchronized block. However, the scope of the synchronized block should not be too small - an atomic operation should not be divided into to more than one synchronized block.\nIt is reasonable to exclude long-running operations that do not affect shared state so that other threads are not prevented from accessing the shared state while the long-running operation is in progress.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  @NotThreadSafe public class UnsafeCachingFactorizer implements Servlet { private final AtomicReference\u0026lt;BigInteger\u0026gt; lastNumber = new AtomicReference\u0026lt;BigInteger\u0026gt;(); private final AtomicReference\u0026lt;BigInteger[]\u0026gt; lastFactors = new AtomicReference\u0026lt;BigInteger[]\u0026gt;(); public void service(ServletRequest req, ServletResponse resp) { BigInteger i = extractFromRequest(req); if (i.equals(lastNumber.get())) encodeIntoResponse(resp, lastFactors.get() ); else { BigInteger[] factors = factor(i); lastNumber.set(i); lastFactors.set(factors); encodeIntoResponse(resp, factors); } } }   The above example has race conditions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  @ThreadSafe public class SynchronizedFactorizer implements Servlet { @GuardedBy(\u0026#34;this\u0026#34;) private BigInteger lastNumber; @GuardedBy(\u0026#34;this\u0026#34;) private BigInteger[] lastFactors; public synchronized void service(ServletRequest req, ServletResponse resp) { BigInteger i = extractFromRequest(req); if (i.equals(lastNumber)) encodeIntoResponse(resp, lastFactors); else { BigInteger[] factors = factor(i); lastNumber = i; lastFactors = factors; encodeIntoResponse(resp, factors); } } }   In the above example, the servlet caches the last result but has unacceptable concurrency. Only one thread can use the servlet at a time, defeating the purpose of using servlets.\n Serialized access: https://jcip.net/   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  @ThreadSafe public class CachedFactorizer implements Servlet { @GuardedBy(\u0026#34;this\u0026#34;) private BigInteger lastNumber; @GuardedBy(\u0026#34;this\u0026#34;) private BigInteger[] lastFactors; @GuardedBy(\u0026#34;this\u0026#34;) private long hits; @GuardedBy(\u0026#34;this\u0026#34;) private long cacheHits; public synchronized long getHits() { return hits; } public synchronized double getCacheHitRatio() { return (double) cacheHits / (double) hits; } public void service(ServletRequest req, ServletResponse resp) { BigInteger i = extractFromRequest(req); BigInteger[] factors = null; synchronized (this) { ++hits; if (i.equals(lastNumber)) { ++cacheHits; factors = lastFactors.clone(); } } if (factors == null) { factors = factor(i); synchronized (this) { lastNumber = i; lastFactors = factors.clone(); } } encodeIntoResponse(resp, factors); } }   By narrowing the scope of the synchronized block, concurrency of the servlet can be improved while maintaining thread safety.\nThe above example uses two synchronized block, each limited to a short section of code.\n One guards the check-then-act that tests whether we can just return the cached result. The other guards the update of cached number and the cached factors.  The portions of code that are outside the synchronized blocks operate exclusively on local variables, which are not shared across threads and therefore do not require synchronization.\nAcquiring and releasing a lock has some overhead, so it is undesirable to break down synchronized blocks too far (even though it would not compromise atomicity).\nCachedFactorizer holds the lock when accessing state variables and for the duration of compound actions but releases it before executing the potentially long-running factorization operation. This preserves thread safety without affecting concurrency.\nwhen implementing a synchronization policy, resist the temptation to prematurely sacrifice simplicity (potentially compromising safety) for the sake of performance. Holding a lock for a long time, either due to compute-intensive tasks or due to a blocking operation, introduces the risk of liveliness or performance problems.\nAvoid holding locks during lengthy computations or operations at risk of not completing quickly such as network or console I/O.\n ","description":"Contains Notes for the book Java concurrency in practice - https://jcip.net/)","id":9,"section":"tech","tags":null,"title":"Ch01 - Thread Safety","uri":"https://adhithyakrishna.github.io/tech/java_concurrency_in_practice/fundamentals/thread_safety/"},{"content":"  Contains Notes for the book Java concurrency in practice - https://jcip.net/)\nBenefits of threads Proper utilization of thread can\n Reduce development and maintenance cost. Improve the performance of complex applications.  Threads are useful in GUI applications for improving the responsiveness of the user interface. In server applications, threads help in improving the resource utilization and throughput.\nSince the basic unit of scheduling is the thread. A single threaded program can run on at most one processor at a time. When a single threaded program is run on a 100 processor system, it is giving up access to 99%.\nWhen designed properly, a multi-threaded program can improve throughput by utilizing available processor resources more effectively.\nUsing multiple threads can help achieve a better throughput on a single processor system as well. When a thread is waiting for the I/O to complete, another thread can still run allowing the application to make progress during the blocking I/O.\nAssigning a thread to each type of task will make it look like its sequential and insulates domain logic from the details of\n Scheduling Interleaved operations Asynchronous I/O Resource waits  A complicated async workflow can be decomposed into a number of simpler synchronous workflows, each running in a separate thread, interacting with each other at a specific synchronization points.\nExample: RMI (Remote Method Invocation):\nThe framework handles the details of\n Request management Thread creation Load balancing Dispatching portions of the request handling to appropriate app component at the appropriate point in the work-flow.  Servlet writers need not worry about how many requests are being processed at the same time or whether the socket input and output streams block. When a servlet\u0026rsquo;s service method is called in response to a web client, it can process the request synchronously as if it were a single threaded program, greatly simplifying the component development and reduce the learning curve of such frameworks.\nA server application that is single threaded would require non-blocking I/O which is more complicated and error-prone than synchronous I/O.\n","description":"Contains Notes for the book Java concurrency in practice - https://jcip.net/)","id":10,"section":"tech","tags":null,"title":"Ch02 - Benefits of Threads","uri":"https://adhithyakrishna.github.io/tech/java_concurrency_in_practice/introduction/benefits_of_threads/"},{"content":"  This notes is for the course Kuberenetes for absolute beginners\nContainer orchestration Kubernetes is a Container Orchestration technology. Docker has its own container orchestration - docker swarm. There is also MESOS from Apache.\nThe process of automatically deploying and managing containers (scaling up when load increases and scaling down when load decreases) is known as Container Orchestration.\nAdvantages of container orchestration\n Application is highly available, since there are multiple instances of the application running on different nodes. User traffic is load balanced across various containers. When demand increases, deploy more instances of the application seamlessly this can be done at the service level. When we run out of hardware, we can scale the number of nodes up/down without having to take down the application. All of the above can be done with a set of declarative object configuration files.  Kubernetes components Nodes Node is a machine (physical or virtual) on which k8s is installed. A node is worker machine and this is going to hold the containers launched by kubernetes.\n K8s Nodes: https://www.udemy.com/course/learn-kubernetes/   Cluster Cluster is a set of nodes grouped together. This way, even if one node fails, the application is still accessible from other nodes.\nMultiple nodes would help with sharing load as well.\nMaster Master is another node with kubernetes installed in it and is configured as a master. The master watches over the nodes in the cluster and is responsible for the actual orchestraction of containers on the worker nodes.\nFew responsibilities of master node\n Responsible for managing the cluster. Stores the information about the members of the cluster. Monitor nodes. Moving the workload of the failed nodes to another worker nodes.  Components of Kubernetes (Control plane) When a k8s is installed on a system, the following components are also installed\n K8s Components: https://www.udemy.com/course/learn-kubernetes/   1) An API server - An API server acts as a front-end for kubernetes. The users, management devices, CLI all talk to the API server to interact with kubernetes.\n2) An ETCD service - It is a reliable key-value store to store all the data used to manage the cluster. ETCD stores the information of multiple nodes and multiple masters in the k8s cluster in a distributed manner. ETCD is also responsible for implementing locks within the cluster to ensure there are no conflicts between the masters.\n3) A Kubelet service - It is an agent that runs on each node in the cluster. It is responsible for making sure that containers are running on the nodes as expected.\n4) A Container runtime - An underlying software that is used to run the containers. (Docker)\n5) Controllers - They are the brain behind the orchestration. They notice and respond when the nodes, containers or endpoints go down. The controllers would make decisions to bring up new containers in such case.\n6) Schedulers - Responsible for distributing work or containers across multiple nodes. It looks for newly created containers and assigns them to Nodes.\nThe following factors are taken into account for scheduling decisions\n Individual and collective resource requirements Hardware/software/policy constraints Affinity and anti-affinity specifications Data locality Inter-workload interference deadlines  Master vs Nodes  Master vs worker nodes: https://www.udemy.com/course/learn-kubernetes/   The master node has\n Kube-apiserver installed ETCD Control manager Scheduler and more.  Worker node has\n Container run time installed. Kubelet agent - provides health information of the worker node to the master and carry out the actions requested by the master on the worker nodes.  Kubectl (kube control) Tool used to deploy and manage applications on a Kubernetes cluster\n Provide cluster information Get status of nodes in the cluster  Pod Kubernetes does not deploy containers directly on the worker nodes. The containers are encapsulated into a kubernetes objects known as pods. A Pod is a single instance of an application and is the smallest object that can be created in kubernetes.\nTo scale up an application, we create a new pod instead of creating additional container inside of an existing pod. A pod can hold additional containers as well (usually not of the same kind). The additional container can be a helper container that supports the main application. The two containers can communicate with each other directly by referring to each other as localhost since they share the same network namespace. They share the same storage space as well.\n","description":"Kubernetes architecture","id":11,"section":"tech","tags":null,"title":"Kubernetes architecture","uri":"https://adhithyakrishna.github.io/tech/k8s_for_absolute_beginners/k8s_architecture/"},{"content":"This article explains embedding interfaces concept in golang. We first begin by writing the main crux of the code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  type Animal struct { Dog } type Dog struct { } func (d Dog) speak() { fmt.Println(\u0026#34;woof\u0026#34;) } func main() { d := Animal{Dog{}} d.speak(); v := Animal{} v.speak(); }   In the above code we declare a type of struct named Dog. There is a method embedded to the Dog struct called speak.\nWe declare a new struct Animal, which has Dog as one of if its fields. Now, All the methods embedded on the struct Dog can be accessed by creating a variable for the struct type Animal (Line 14).\nThough this works fine, the problem here is, the struct Animal has a field Dog hard coded to it. Suppose we have to include a cat, we have to then alter the struct Animal to include cat. It doesnt end there, we\u0026rsquo;ll also have to modify the variableinitializationn to include Cat. Now, since both dog and cat has the function speak we will have to explicitly specify which function are we intending to call. Code including cat is given below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  type Animal struct { Dog Cat } type Dog struct { } func (d Dog) speak() { fmt.Println(\u0026#34;woof\u0026#34;) } type Cat struct { } func (c Cat) speak() { fmt.Println(\u0026#34;meow\u0026#34;) } func main() { d := Animal{ Dog{}, Cat{}, } d.Cat.speak(); }   To make it easy for us to swap between different animals, or include multiple animals, we can make use of Interfaces.\nInterfaces Step 1: Create a type of interface that encompasses the common functionalites. 1 2 3  type Language interface { speak() }   Now, any type that implements speak function is an implementation of the Language interface.\nStep 2: Create a type of struct Dog, and implement the functions of the interface. 1 2 3 4 5  type Dog struct {} func (d Dog) speak() { fmt.Println(\u0026#34;Woof\u0026#34;) }   Step 3: Include the interface as a field to the Animal struct 1 2 3  type Animal struct { Language }   Now, any struct that implements the speak function can be initialized to the Language field during declaration. We need not disturb the Animal struct again.\nInvoking the speak function for Dog is just a matter of initializing the variable with the struct Animal with any one of the implementations of the Language interface as below.\n1 2  d := Animal{Dog{}} d.speak();   Including a new animal is easy. All we have to do is, create a new struct (Ex. Cat), embed a function speak() to the struct.\n1 2 3 4 5  type Cat struct {} func (d Cat) speak() { fmt.Println(\u0026#34;Woof\u0026#34;) }   The real world advantage to doing this is that, now any time we decide to replace a functionality, say, we have included some customer specific logic, it is as easy as swapping it with the new struct that implements the interface.\nFull code can be found below\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  import \u0026#34;fmt\u0026#34; type Animal struct { Language } type Dog struct {} func (d Dog) speak() { fmt.Println(\u0026#34;Woof\u0026#34;) } type Cat struct {} func (d Cat) speak() { fmt.Println(\u0026#34;Meow\u0026#34;) } type Language interface { speak() } func main() { d := Animal{Dog{}} d.speak(); c := Animal{Cat{}} c.speak(); }   In line 23, Dog can be replaced by Cat and just by swapping, we can include functionality of cat instead of dog.\nThe output is\nWoof\nMeow\nFind the source code here https://gist.github.com/adhithyakrishna/ac72d0d4af806b764d66fda8efec8728 ","description":"An article about embedding in golang","id":12,"section":"tech","tags":null,"title":"Embedding Interfaces","uri":"https://adhithyakrishna.github.io/tech/golang/embeddinginterfaces/"},{"content":"  Contains Notes for the book Java concurrency in practice - https://jcip.net/)\nIn the absence of synchronization, the ordering of operations in multiple threads is unpredictable.\nSafety Hazards 1 2 3 4 5 6 7  public class UnsafeSequence { private int value; /** Returns a unique value. */ public int getNext() { return value++; } }   In the above example,value++ is three separate operations.\n Read the value Add one to it Write the new value.  Two threads can call getNext and receive the same value since operations in multiple threads may be arbitrarily interleaved by the runtime. The result is that same sequence number is returned from multiple calls in different threads.\n Safety hazards: Unsafe sequence example   The above unsafe sequence example illustrate the common concurrency hazard called race condition.\nSince threads share the same memory address space and run concurrently, they can access or modify variables that other threads might be using. Convenience here is, it makes data sharing much easier compared to other inter-thread communication mechanisms. The risk here is, threads can be confused by having data change unexpectedly.\nAllowing multiple threads to access and modify the same variables will introduce the element of non sequentially into an otherwise sequential programming model.\nFor a multi-threaded program to be predictable, access to shared variables must be properly coordinated so that the threads do not interfere with one another.\n1 2 3 4 5 6  public class Sequence { private int nextValue; public synchronized int getNext() { return nextValue++; } }   Unsafe-sequence can be fixed by making getNext() a synchronized method.\nIn absence of synchronization, the compiler, hardware and runtime are allowed to take substantial liberties with the timing and ordering of actions such as caching variables in registers or processor-local caches. These tricks are in aid of better performance and are generally desirable but developer has to clearly identify where the data is being shared across threads so these optimizations don\u0026rsquo;t undermine safety.\nLiveness Hazards Use of threads introduces additional form of liveliness failure.\nSafety -\u0026gt; nothing bad ever happens\nliveliness -\u0026gt; something good eventually happens.\nA liveliness failure occurs when an activity gets into a state such that it is permanently unable to make forward progress. Example deadlock, starvation and livelock.\nPerformance Hazards Multi-threaded programs are subject to all performance hazards of single threaded programs and to others as well that are introduced by the use of threads.\nContext-switches when the scheduler suspends the active thread temporarily so another thread can run - have significant costs. Such as\n Saving and restoring execution context Loss of locality CPU time spent scheduling threads instead of running them.  When threads share data, they must use synchronization mechanisms that can inhibit\n Compiler optimizations Flush Invalidate memory caches Create synchronization traffic on memory bus.  All of the above factors introduce additional performance costs.\nThreads are Everywhere Every java app uses threads. When JVM starts, it creates threads for JVM house keeping tasks and main thread for running main method.\nWhen concurrency is introduced into an application by a framework, it is usually impossible to restrict the concurrency awareness to the framework code. Framework by nature make callbacks to the application components that in turn access the application state.\nSimilarly the need for thread safety does not end with the components called by the framework - it extends to all code paths that access the program state accessed by the components. Thus the need for thread safety is contagious.\nObjects accessed by the tasks themselves should be made thread-safe, encapsulating the thread safety within the shared objects.\n","description":"Contains Notes for the book Java concurrency in practice - https://jcip.net/)","id":13,"section":"tech","tags":null,"title":"Ch03 - Risks of threads","uri":"https://adhithyakrishna.github.io/tech/java_concurrency_in_practice/introduction/risks_of_threads/"},{"content":"  This notes is for the course Kuberenetes for absolute beginners\nYaml introduction According to yaml.org, \u0026ldquo;YAML is a human-friendly, data serialization standard for all programming languages.\u0026quot;\nYaml is used to create kubernetes configurations\nStructure of YAML file Key Value Pair The basic type of entry in a YAML file is of a key value pair. After the Key and colon there is a space and then the value.\nFruit: Apple Vegetable: Radish Liquid: Water Array/List Lists would have a name and a number of items listed under it. The elements of the list would start with a -. There can be a n of lists, however the indentation of various elements of the array matters a lot.\nFruits: - Orange - Banana - Mango Vegetables: - Potato - Tomato - Carrot Dictionary/Map A more complex type of YAML file would be a Dictionary/Map.\nBanana: Calories: 200 Fat: 0.5g Carbs: 30g Grapes: Calories: 100 Fat: 0.4g Carbs: 20g It is important to indent Yaml properly. Improper indentation would break a sibling / parent relationship between the properties. Advanced YAML structures List containing a list of dictionaries. - Fruits: - Banana: Calories: 105 Fat: 0.4g Carbs: 27g - Grape: Calories: 62 Fat: 0.3g Carbs: 16g - Vegetables: - Potato: Calories: 105 Fat: 0.4g Carbs: 27g Dictionary in Dictionary Banana: type: name: musa Calories: 200 Fat: 0.5g Carbs: 30g A list is ordered\nA dictionary is unordered ","description":"Yaml Introduction","id":14,"section":"tech","tags":null,"title":"Yaml Introduction","uri":"https://adhithyakrishna.github.io/tech/k8s_for_absolute_beginners/yaml_introduction/"},{"content":"This article is a continuation to the previous article about Embedding interfaces found here\nWe start from the crux of the previous code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  import \u0026#34;fmt\u0026#34; type Animal struct { Language } type Dog struct {} func (d Dog) speak() { fmt.Println(\u0026#34;Woof\u0026#34;) } type Cat struct {} func (d Cat) speak() { fmt.Println(\u0026#34;Meow\u0026#34;) } type Language interface { speak() } func main() { d := Animal{Dog{}} d.speak(); c := Animal{Cat{}} c.speak(); }   Now, Let us say, we work for a client who wants add a functionality. It may be something like, adding a prefix to the language spoken by the animal.\nTo do that, we can make use of the concept called interface chaining.\nDeclare a type struct that includes the interface as one of its field. It should also include implementations of the speak function. 1 2 3 4 5 6 7 8  type Initiator struct { Language } func (i Initiator) speak() { fmt.Print(\u0026#34;The animal says : \u0026#34;, i) i.Language.speak() }   Now Initiator struct implements the language interface. Since it includes, Language interface as one of its fields, chaining is very easy to do.\n1 2  c := Animal{Initiator{Cat{}}} c.speak()   The Initiator functionality has been chained into the variable declaration. Now everytime a speak function is called on the intialised variable, first, the speak function in the initiator struct would be called and then, the speak function in the Cat struct would take place.\nFull code is given below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  package main import \u0026#34;fmt\u0026#34; type Animal struct { Language } type Dog struct{} func (d Dog) speak() { fmt.Println(\u0026#34;Woof\u0026#34;) } type Cat struct{} func (d Cat) speak() { fmt.Println(\u0026#34;Meow\u0026#34;) } type Language interface { speak() } type Initiator struct { Language } func (i Initiator) speak() { fmt.Print(\u0026#34;The animal says : \u0026#34;) i.Language.speak() } func main() { d := Animal{Dog{}} d.speak() c := Animal{Initiator{Cat{}}} c.speak() }   The output is\nWoof\nThe animal says : Meow\nFind the source code here https://gist.github.com/adhithyakrishna/ad5cb9b6f4ce1d5bb98407f9502e51ee ","description":"An article about Chaining interfaces in golang","id":15,"section":"tech","tags":null,"title":"Chaining Interfaces","uri":"https://adhithyakrishna.github.io/tech/golang/chaininginterfaces/"},{"content":"  This notes is for the course Kuberenetes for absolute beginners\nKubernetes definition file A Kubernetes definition file always contains 4 top level fields. These are required fields\napiVersion: kind: metadata: . . . spec:  apiVersion - Version of k8s API kind - type of object (pod, replica-set, service, deployment) metadata - data about the object (name and label), it is in the form of a dictionary. Here name is a string value and labels is a dictionary. Labels are useful to identify objects at a later point in time.  metadata: name: myapp-pod labels: app: myapp spec - (specification) where we provide additional information to k8s pertaining to that object. Spec is a dictionary, we can add properties to it, containers in this example, which is a list. The reason this property is a list is because the PODs can have multiple containers within them.  spec: containers: - name: nginx-container image: nginx Few of the kubectl commands kubectl create -f pod-definition.yml kubectl get pods kubectl describe pods ","description":"Yaml for Kubernetes","id":16,"section":"tech","tags":null,"title":"Yaml for Kubernetes","uri":"https://adhithyakrishna.github.io/tech/k8s_for_absolute_beginners/yaml_for_k8s/"},{"content":"Golang lets us declare a variable of type functions. In Golang functions are first class citizens. In this article we are going to see how the functions can be used a type in golang.\nDeclaration of the variable 1  type validator func(*User) error   Validator is the name of the variable that has a type function which takes the struct User as its argument and returns an error.\nDeclaration of struct and functions on the struct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  type userValidator struct {} func (uv *userValidator)validateEmail(u *User) error { matched, _ := regexp.MatchString(`^[a-z0-9._%+\\-]+@[a-z0-9.\\-]+\\.[a-z]{2,16}$`, u.email) if !matched { return ErrEmailInvalid } return nil } func (uv *userValidator) isEmpty(u *User) error { if u.email == \u0026#34;\u0026#34; { return ErrEmailEmpty } return nil }   If we observe, we can note that the function argument as well the return type matches the variable we declared above. Now we can pass validator as an argument to different functions and execute them. Below is an example.\n1 2 3 4 5 6 7 8  func runUserValidations(user *User, fns ...validator) error { for _, fns := range fns { if err:=fns(user); err != nil { return err } } return nil }   You can see that, validator is passed as an argument to the runUserValidations functions. The function can be executed by passing an instance of user struct as an argument.\nI have included an example below of how using function types will come in handy while doing multiple functions that validates if an email is valid.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58  package main import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;regexp\u0026#34; ) var ( ErrEmailInvalid = errors.New(\u0026#34;email address is not valid\u0026#34;) ErrEmailEmpty = errors.New(\u0026#34;email address is empty\u0026#34;) ) type User struct { email string } type userValidator struct {} type validator func(*User) error func (uv *userValidator)validateEmail(u *User) error { matched, _ := regexp.MatchString(`^[a-z0-9._%+\\-]+@[a-z0-9.\\-]+\\.[a-z]{2,16}$`, u.email) if !matched { return ErrEmailInvalid } return nil } func (uv *userValidator) isEmpty(u *User) error { if u.email == \u0026#34;\u0026#34; { return ErrEmailEmpty } return nil } func runUserValidations(user *User, fns ...validator) error { for _, fns := range fns { if err:=fns(user); err != nil { return err } } return nil } func main() { u := User{ email: \u0026#34;adhithya.awesome@gmail.com\u0026#34;, } uv := userValidator{} err:= runUserValidations(\u0026amp;u, uv.isEmpty, uv.validateEmail) if err != nil { fmt.Println(err) return } fmt.Println(\u0026#34;valid email\u0026#34;) }   If you note line number 52 in the above code you can see how an instance of user struct, initialized with an email is passed to different functions, validated and if there is an error in the validation it prints the error or it prints the email is valid.\nWe can included multiple such functions and calling them is a matter of including them to the runUserValidations functions.\n","description":"An article about function types in golang","id":17,"section":"tech","tags":null,"title":"Function types","uri":"https://adhithyakrishna.github.io/tech/golang/functiontypes/"},{"content":"  This notes is for the course Kuberenetes for absolute beginners\nReplication controller  Replication controller helps us run multiple instance of a pod in kubernetes cluster, thus providing high availability. Replication controller is capable of ensuring that the specified number of pods are running at all times. It is also capable of load balancing between multiple pods to share the loads. It can load balance pods and can do it when pods are in different nodes as well. It allows us to scale the application when demand increases.  apiVersion: v1 kind: ReplicationController metadata: name: myapp-rc labels: app: myapp type: front-end spec: template: metadata: name: myapp-pod labels: app: myapp type: frontend spec: containers: - name: nginx-container image: nginx replicas: 3 \u0026gt; kubectl get replicationcontroller Replica Set Replication controller that we saw previously is being replaced by replica set. Replica Set is the recommended way to setup replication.\nOne difference between Replication Controller and Replica Set is, Replica Set requires a selector definition to identify what pods fall under it. Replica Set will take those pods that are already deployed that match the selector definition when creating the replicas.\nReplica Set can also be used to monitor the existing pods and redploy if one of them fails.\nLabeling our pods is essential because there may be hundreds of pods running in our cluster and we can use labels and selectors to identify the pods.\nIf one of the pods fails, Replica Set will use the configuration under the spec property to re-deploy the pod.\napiVersion: apps/v1 kind: ReplicaSet metadata: name: myapp-replicaset labels: app: myapp type: front-end spec: template: metadata: name: myapp-pod labels: app: myapp type: front-end spec: containers: - name: nginx-container image: nginx replicas: 3 selector: matchLabels: type: front-end kubectl create -f replicaset-definition.yml kubectl get replicaset kubectl delete replicaset myapp-replicaset Scaling the replicaset One of the following commands can be used to scale up the replicaset\nkubectl replace -f replicaset-definition.yml kubectl scale --replicas=6 -f replicaset-definition.yml kubectl scale --replicas=6 replicaset myapp-replicaset Deployments  Deployment provides declarative updates for Pods and ReplicaSets When you describe a desired state in a deployment, the deployment controller is capable of changing the actual state to the desired state at a controlled rate. You can define Deployments to create a new ReplicaSets or to remove existing Deployments and adopt all their resources with new Deployments.  Deployments have several interesting use cases which are listed in this page. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\nDeployment lifecycle First the deployment is created.\nDeployment in turn creates the replicaset.\nReplica Set has in turn creates pods.\nKubectl describe deployment command can be used to find more information about the deployment status.\napiVersion: apps/v1 kind: Deployment metadata: name: myapp-replicaset labels: app: myapp type: front-end spec: template: metadata: name: myapp-pod labels: app: myapp type: front-end spec: containers: - name: nginx-container image: nginx replicas: 3 selector: matchLabels: type: front-end kubectl get all Deployment Rollout and Versioning Whenever a new deployment or upgrade an image to the existing deployment, a roll out is triggered. A roll out is the process of gradually deploying or upgrading your application containers.\nWhen a new deployment is created, it triggers a roll out. A new roll out creates a new Deployment revision (Version 1). When the application is upgraded later, a new deployment revision is created (Version 2).\nThis will help us keep track of the changes made to our deployment and enable us to rollback to the previous version of the deployment if necessary.\nkubectl rollout status kubectl rollout history Deployment Strategies The default deployment strategy is Roll-out. There is another strategy called Recreate which as the name implies, deletes all the pods and recreates them bringing the entire service down while doing it. Roll-out strategy on the other hand, takes down the older version and brings back the newer version one by one. This way, the application never goes down and the upgrade is seamless.\nTo update the image of the running deployment kubectl set image deployment/myapp-deployment nginx-container=nginx:1.12-perl command can be used.\nDeployment Upgrades When a new deployment is created, it creates a Replica-Set which in turn creates the number of Pods required to meet the number of replicas. When you upgrade your application, the kubernetes deployment creates a NEW replica set under the hood and starts deploying the containers there. Taking down the pods in old Replica Set also follow the Rolling Update strategy.\nDeployment rollbacks kubectl rollout undo followed by the name of the deployment will allow you to rollback to a previous version. The deployment will destroy the pods in new replicaset and bring back the older ones in the old replicaset.\nkubectl run nginx --image=nginx the kubectl run command actually creates a deployment. A replicaset and pods are automatically created in the backend.\nList of useful commands \u0026gt; kubectl create -f deployment definition.yml \u0026gt; kubectl create -f deployment definition.yml --record \u0026gt; kubectl get deployments \u0026gt; kubectl apply -f deployment definition.yml \u0026gt; kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1 \u0026gt; kubectl rollout status deployment/myapp-deployment \u0026gt; kubectl rollout history deployment/myapp-deployment \u0026gt; kubectl rollout undo deployment/myapp deployment ","description":"Kubernetes controllers","id":18,"section":"tech","tags":null,"title":"Kubernetes controllers","uri":"https://adhithyakrishna.github.io/tech/k8s_for_absolute_beginners/k8s_controllers/"},{"content":"  This notes is for the course Kuberenetes for absolute beginners\nKubernetes service enable communication between various components within or ourside of the application. Kubernetes Services helps us connect applications with other applications or users.\nServices enable connectivity between the group of pods. For example front-end to the users, connection between the frontend and backend processes and backend to to external data source.\nServices enable Loose coupling between micro services in our application.\nNode Port One of the use case of Nodeport is to listen to a port on the Node and forward requests on that port to a port on the pod running the web application.\nThis type of service is know as NodePort service because the service listens to a port on the Node and forwards requests to Pods.\nNodePorts can only be in a valid range which is from 30000 to 32767.\n K8s Services: https://www.udemy.com/course/learn-kubernetes/  \nBelow terms are from the viewpoint of the service. The service is like a virtual server inside of the node. It has its own ip address.\n TargetPort - The port on the Pod where the actual web server is running - (port 80). This is where the service forwards requests to. Port - Port on the service itself. (Simply referred to as the port). NodePort - Port on the node, which can be used to access the web server externally.  apiVersion: v1 kind: Service metadata: name: spring-boot-service namespace: spring-boot spec: selector: app: spring-boot-deployment ports: - port: 8080 targetPort: 8080 nodePort: 30009 type: NodePort Here the only mandatory field is port. The target port would be the same as port if not specified and nodePort value will be between the 30000 and 32767 range (automatically allocated if not specified).\nWe can do multiple port mappings within a single service.\nLabels and selectors are used to map the pods to the service. When a service is created it looks for matching pods with the labels and finds 3 of them. The service then auto selects all 3 pods to forward the external requests coming from the user. No additional configuration is needed to make this happen.\nThis service acts as a built in load balancer to distribute the load across different pods.\n K8s Services: https://www.udemy.com/course/learn-kubernetes/   Kubernetes creates a service that can spans across all the nodes in the cluster and maps the target port to the same nodeport on all the nodes in the cluster, without having to do any additional configurations.\nThis way the application can be accessed using the IP of any node by using the same port number.\nTo summarize – in ANY case whether it be a single pod in a single node, multiple pods on a single node, multiple pods on multiple nodes, the service is created exactly the same without having to do any additional steps during the service creation.\nWhen PODs are removed or added the service is automatically updated making it highly flexible and adaptive. Once created, you won’t typically have to make any additional configuration changes.\nClusterIP ClusterIP provides a single interface to access pods in a group. A service created for back-end pods will help group all the back-end pods and provide a single interface for the other pods to access this service, allowing to easily and effectively deploy a micro-services based application on a Kubernetes cluster. Similarly, creating additional services for database layer allows the back-end pods to access the database layer through this service.\nEach service gets an IP and name assigned to it inside the cluster and that is the name that should be used by the other pods to access the service. This type of service is known as ClusterIP. ClusterIP is the default type in kubernetes configuration. If we did not specify the type, ClusterIP would be assigned by default.\n K8s Services: https://www.udemy.com/course/learn-kubernetes/   apiVersion: v1 kind: Service metadata: name: back-end spec: type: ClusterIP ports: - targetPort: 80 port: 80 selector: app: myapp type: back-end LoadBalancer Exposes the Service externally using a cloud provider\u0026rsquo;s load balancer.\nOn cloud providers which support external load balancers, setting the type field to LoadBalancer provisions a load balancer for your Service.\napiVersion: v1 kind: Service metadata: name: my-service spec: selector: app.kubernetes.io/name: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 clusterIP: 10.0.171.239 type: LoadBalancer status: loadBalancer: ingress: - ip: 192.0.2.127 Differrence between different services https://stackoverflow.com/questions/41509439/whats-the-difference-between-clusterip-nodeport-and-loadbalancer-service-types\nExcerpts from the above stack overflow link\nYou can access a service from your load balancer\u0026rsquo;s IP address, which routes your request to a nodePort, which in turn routes the request to the clusterIP port. You can acess this service as you would a NodePort or a ClusterIP service as well.\nA ClusterIP Service is part of a NodePort Service. A NodePort Service is Part of a Load Balancer Service.\nkubectl get services displays the loadbalancer as well as cluster-ip\nminikube service redis-service --url displays the node-ports.\n K8s Services: https://www.udemy.com/course/learn-kubernetes/  \nClusterIP - Exposes a service which is only accessible from within the cluster.\nNodePort - Exposes a service via a static port on each node’s IP.\nLoadBalancer - Exposes the service via the cloud provider’s load balancer.\nPods within the cluster can talk to each other through clusterIP.\nTo make a pod accessible from outside the cluster, it will create nodePort. Node port will make use of the clusterIP to do this.\nLoad balancer puts a loadbalancer in front so that the inbound traffic is distributed between node ports.\nIf you want to access the service from outside a cluster only Nodeport will be accessible and not clusterIP.\nAdditional reference: https://stackoverflow.com/a/72988866\n","description":"Kubernetes services","id":19,"section":"tech","tags":null,"title":"Kubernetes services","uri":"https://adhithyakrishna.github.io/tech/k8s_for_absolute_beginners/k8s_services/"},{"content":"  This notes is for the course Kuberenetes for absolute beginners\nApplication flow  K8s Pods demo: https://www.udemy.com/course/learn-kubernetes/   Voting application We create pods and expose the container port. Please note that all the docker images are pre-built and this demo is to understand how different microservices communicate with each other through k8s services.\nCreate a definition for pod voting-app-pod.yml\napiVersion: v1 kind: Pod metadata: name: voting-app-pod labels: name: voting-app-pod app: demo-voting-app spec: containers: - name: voting-app image: dockersamples/examplevotingapp_vote imagePullPolicy: IfNotPresent ports: - containerPort: 80 restartPolicy: Always Create a Loadbalancer service to expose the application to external world voting-app-service.yml\napiVersion: v1 kind: Service metadata: name: voting-service labels: name: voting-service app: demo-voting-app spec: ports: - port: 80 targetPort: 80 selector: name: voting-app-pod app: demo-voting-app type: LoadBalancer Result application Create a definition for a pod result-app-pod.yml\napiVersion: v1 kind: Pod metadata: name: result-app-pod labels: name: result-app-pod app: demo-voting-app spec: containers: - name: result-app image: dockersamples/examplevotingapp_result imagePullPolicy: IfNotPresent ports: - containerPort: 80 restartPolicy: Always Create a Loadbalancer service to expose the application to external world result-app-service.yml\napiVersion: v1 kind: Service metadata: name: result-service labels: name: result-service app: demo-voting-app spec: ports: - port: 80 targetPort: 80 selector: name: result-app-pod app: demo-voting-app type: LoadBalancer Redis application Create a definition for a pod redis-pod.yml\napiVersion: v1 kind: Pod metadata: name: redis-pod labels: name: redis-pod app: demo-voting-app spec: containers: - name: redis image: redis imagePullPolicy: IfNotPresent ports: - containerPort: 6379 restartPolicy: Always Create a NodePort service for postgres redis-service.yml\napiVersion: v1 kind: Service metadata: name: redis labels: name: redis-service app: demo-voting-app spec: ports: - port: 6379 targetPort: 6379 selector: name: redis-pod app: demo-voting-app type: NodePort Services are created so that the pods can communicate to each other.\nServices should be named based on what other applications are looking for. The name here is set to db because the application looks for service named db (you can see that in the code).\n Postgres application Create a definition for a pod postgres-pod.yml\napiVersion: v1 kind: Pod metadata: name: postgres-pod labels: name: postgres-pod app: demo-voting-app spec: containers: - name: postgres image: postgres:9.4 imagePullPolicy: IfNotPresent env: - name: POSTGRES_USER value: \u0026quot;postgres\u0026quot; - name: POSTGRES_PASSWORD value: \u0026quot;postgres\u0026quot; - name: POSTGRES_HOST_AUTH_METHOD value: trust ports: - containerPort: 5432 restartPolicy: Always Create a NodePort service for postgres postgres-service.yml\napiVersion: v1 kind: Service metadata: name: db labels: name: db-service app: demo-voting-app spec: ports: - port: 5432 targetPort: 5432 selector: name: postgres-pod app: demo-voting-app type: NodePort Worker application Note that these pod is needed just for internal communication.\nCreate a definition for a pod worker-app-pod.yml\napiVersion: v1 kind: Pod metadata: name: worker-app-pod labels: name: worker-app-pod app: demo-voting-app spec: containers: - name: worker-app image: dockersamples/examplevotingapp_worker imagePullPolicy: IfNotPresent restartPolicy: Always Output  K8s Pods demo: https://www.udemy.com/course/learn-kubernetes/   Application communication (in docker) Note that links is deprecated but this gives an idea of how the applications communicate internally.  K8s Pods demo: https://www.udemy.com/course/learn-kubernetes/   ","description":"Kubernetes services","id":20,"section":"tech","tags":null,"title":"Demo - Voting application 1","uri":"https://adhithyakrishna.github.io/tech/k8s_for_absolute_beginners/k8s_pods_demo/"}]